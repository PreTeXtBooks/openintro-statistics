<?xml version="1.0" encoding="UTF-8"?>
<chapter xml:id="ch-multiple-logistic-regression" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Multiple and Logistic Regression</title>
  
  <introduction>
    <p>
      The principles of simple linear regression lay the foundation for more sophisticated
      regression models used in a wide range of challenging settings. In this chapter, we
      explore multiple regression, which introduces the possibility of more than one predictor
      in a linear model, and logistic regression, a technique for predicting categorical
      outcomes with two levels.
    </p>
  </introduction>
  
  <!-- Section 9.1: Introduction to multiple regression -->
  <section xml:id="sec-multiple-regression-intro">
    <title>Introduction to Multiple Regression</title>
    
    <introduction>
      <p>
        Multiple regression extends simple two-variable regression to the case that still has
        one response but many predictors (denoted <m>x_1, x_2, x_3, \ldots</m>). The method is
        motivated by scenarios where many variables may be simultaneously connected to an output.
      </p>
    </introduction>
    
    <subsection xml:id="subsec-indicator-categorical-predictors">
      <title>Indicator and Categorical Variables as Predictors</title>
      
      <p>
        In multiple regression, we can include both quantitative and categorical variables as
        predictors. Indicator variables (also called dummy variables) allow us to include
        categorical information in a linear regression model.
      </p>
      
      <p>
        An <term>indicator variable</term> takes a value of 1 when the condition is true and
        0 when it is false. For a categorical variable with more than two levels, we create
        multiple indicator variables, one for each level except one. The omitted level is called
        the <term>reference level</term>.
      </p>
      
      <example xml:id="ex-indicator-variable">
        <statement>
          <p>
            Consider a categorical variable such as income verification status with three levels:
            verified, source only, and not verified. In a regression model, we would create two
            indicator variables, leaving one level as the reference.
          </p>
        </statement>
      </example>
    </subsection>
    
    <subsection xml:id="subsec-multiple-regression-equation">
      <title>Multiple Regression Equation</title>
      
      <p>
        The multiple regression model with <m>k</m> predictors is:
        <me>
          \hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \cdots + b_k x_k
        </me>
        where <m>b_0</m> is the intercept and <m>b_1, b_2, \ldots, b_k</m> are the slopes for
        each predictor.
      </p>
      
      <p>
        The interpretation of <m>b_i</m> is: for each one-unit increase in <m>x_i</m>, we expect
        <m>y</m> to increase by <m>b_i</m> units, holding all other predictors constant.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-interpretation-coefficients">
      <title>Interpreting Coefficients</title>
      
      <p>
        The key to interpreting multiple regression coefficients is the phrase <q>holding all
        other variables constant.</q> This is important because in the real world, variables
        are often correlated, but multiple regression allows us to isolate the effect of each
        variable.
      </p>
      
      <definition xml:id="def-coefficient-interpretation">
        <statement>
          <p>
            In multiple regression, the coefficient <m>b_i</m> for predictor <m>x_i</m> represents
            the expected change in the response variable for a one-unit increase in <m>x_i</m>,
            while holding all other predictors constant.
          </p>
        </statement>
      </definition>
    </subsection>
  </section>
  
  <!-- Section 9.2: Model selection -->
  <section xml:id="sec-model-selection">
    <title>Model Selection</title>
    
    <introduction>
      <p>
        With multiple regression, we have the flexibility to include many predictors, but we must
        decide which variables to include in our model. Including too many variables can lead to
        overfitting, while including too few may leave out important information.
      </p>
    </introduction>
    
    <subsection xml:id="subsec-r-squared-adjusted">
      <title>R-squared and Adjusted R-squared</title>
      
      <p>
        Recall that <m>R^2</m> represents the proportion of variation in the response variable that
        is explained by the predictors. However, <m>R^2</m> increases whenever we add a new variable,
        even if that variable is not truly helpful.
      </p>
      
      <definition xml:id="def-adjusted-r-squared">
        <statement>
          <p>
            <term>Adjusted R-squared</term> is a modified version of <m>R^2</m> that accounts for
            the number of predictors in the model:
            <me>
              R^2_{adj} = 1 - \frac{SSE/(n-k-1)}{SST/(n-1)}
            </me>
            where <m>n</m> is the number of observations and <m>k</m> is the number of predictors.
          </p>
        </statement>
      </definition>
      
      <p>
        Adjusted <m>R^2</m> can decrease if we add a variable that does not improve the model
        sufficiently to offset the penalty for adding parameters.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-variable-selection">
      <title>Variable Selection Strategies</title>
      
      <p>
        Several approaches exist for selecting which variables to include in a multiple regression
        model:
      </p>
      
      <ul>
        <li>
          <p>
            <term>Backward elimination:</term> Start with all variables and remove the least
            significant one at each step.
          </p>
        </li>
        <li>
          <p>
            <term>Forward selection:</term> Start with no variables and add the most significant
            one at each step.
          </p>
        </li>
        <li>
          <p>
            <term>Stepwise selection:</term> A combination of forward and backward approaches.
          </p>
        </li>
        <li>
          <p>
            <term>Best subset selection:</term> Evaluate all possible combinations of predictors.
          </p>
        </li>
      </ul>
    </subsection>
  </section>
  
  <!-- Section 9.3: Checking model assumptions -->
  <section xml:id="sec-model-assumptions">
    <title>Checking Model Assumptions</title>
    
    <introduction>
      <p>
        Like simple linear regression, multiple regression relies on several key assumptions. We can
        check these assumptions using residual plots and other diagnostic tools.
      </p>
    </introduction>
    
    <subsection xml:id="subsec-residual-diagnostics">
      <title>Residual Diagnostics</title>
      
      <p>
        The residuals (observed values minus predicted values) should:
      </p>
      
      <ul>
        <li>
          <p>
            Be approximately normally distributed
          </p>
        </li>
        <li>
          <p>
            Have constant variance (homoscedasticity)
          </p>
        </li>
        <li>
          <p>
            Show no patterns when plotted against fitted values
          </p>
        </li>
        <li>
          <p>
            Be independent of each other
          </p>
        </li>
      </ul>
      
      <p>
        We can assess these assumptions using:
      </p>
      
      <ul>
        <li>
          <p>
            Scatter plots of residuals vs. fitted values
          </p>
        </li>
        <li>
          <p>
            Q-Q plots for normality
          </p>
        </li>
        <li>
          <p>
            Histograms of residuals
          </p>
        </li>
        <li>
          <p>
            Scale-location plots for constant variance
          </p>
        </li>
      </ul>
    </subsection>
  </section>
  
  <!-- Section 9.4: Case study -->
  <section xml:id="sec-regression-case-study">
    <title>Case Study: Mario Kart</title>
    
    <p>
      A case study on the relationship between various characteristics of Mario Kart video game
      items and their prices on the auction website eBay demonstrates the principles of multiple
      regression in practice.
    </p>
    
    <subsection xml:id="subsec-case-study-data">
      <title>The Data</title>
      
      <p>
        The dataset includes information about completed eBay auctions of Mario Kart games,
        with variables such as:
      </p>
      
      <ul>
        <li><p>Price (response variable)</p></li>
        <li><p>Condition of the item (new vs. used)</p></li>
        <li><p>Game system (Wii, Nintendo 64, etc.)</p></li>
        <li><p>Number of wheels/characters included</p></li>
        <li><p>Auction duration</p></li>
      </ul>
    </subsection>
    
    <subsection xml:id="subsec-case-study-analysis">
      <title>Model Development</title>
      
      <p>
        We can build a multiple regression model to predict Mario Kart prices, starting with all
        reasonable predictors and using model selection techniques to arrive at a final model.
      </p>
    </subsection>
  </section>
  
  <!-- Section 9.5: Introduction to logistic regression -->
  <section xml:id="sec-logistic-regression">
    <title>Introduction to Logistic Regression</title>
    
    <introduction>
      <p>
        When the response variable is binary (two categories), linear regression is not appropriate
        because it can produce predicted values outside the range [0, 1]. Logistic regression is
        designed specifically for binary response variables.
      </p>
    </introduction>
    
    <subsection xml:id="subsec-logistic-function">
      <title>The Logistic Function</title>
      
      <p>
        The logistic function provides a smooth curve that constrains predictions to be between 0 and 1:
        <me>
          \hat{p} = \frac{e^{b_0 + b_1 x}}{1 + e^{b_0 + b_1 x}}
        </me>
      </p>
      
      <p>
        This ensures that predicted probabilities are always between 0 and 1, which is crucial for
        modeling probabilities.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-odds-log-odds">
      <title>Odds and Log-Odds</title>
      
      <definition xml:id="def-odds">
        <statement>
          <p>
            The <term>odds</term> of an event occurring are defined as:
            <me>
              \text{odds} = \frac{p}{1-p}
            </me>
            where <m>p</m> is the probability of the event.
          </p>
        </statement>
      </definition>
      
      <p>
        The <term>log-odds</term> (also called the logit) is the natural logarithm of the odds:
        <me>
          \log\left(\frac{p}{1-p}\right) = b_0 + b_1 x
        </me>
      </p>
      
      <p>
        The relationship between <m>x</m> and the log-odds is linear, which makes logistic regression
        a natural extension of multiple linear regression.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-coefficient-interpretation-logistic">
      <title>Interpreting Logistic Regression Coefficients</title>
      
      <p>
        The coefficient <m>b_1</m> in logistic regression represents the change in log-odds for
        a one-unit increase in <m>x</m>. We can interpret this more intuitively by exponentiating:
      </p>
      
      <definition xml:id="def-odds-ratio">
        <statement>
          <p>
            The <term>odds ratio</term> is <m>e^{b_1}</m>, which represents the multiplicative change
            in odds for a one-unit increase in <m>x</m>.
          </p>
        </statement>
      </definition>
    </subsection>
    
    <subsection xml:id="subsec-logistic-inference">
      <title>Inference in Logistic Regression</title>
      
      <p>
        We can perform hypothesis tests and construct confidence intervals for logistic regression
        coefficients using the standard error of the estimate. The test statistic follows a normal
        distribution for large samples.
      </p>
    </subsection>
  </section>
  
  <!-- Chapter review -->
  <section xml:id="sec-ch09-review">
    <title>Chapter Review</title>
    
    <subsection xml:id="subsec-ch09-summary">
      <title>Summary</title>
      
      <p>
        In this chapter, we extended regression methods to handle multiple predictors and binary
        response variables. Key concepts include:
      </p>
      
      <ul>
        <li>
          <p>
            Multiple regression allows us to model relationships with multiple predictors
          </p>
        </li>
        <li>
          <p>
            Adjusted R-squared helps balance model complexity with explanatory power
          </p>
        </li>
        <li>
          <p>
            Residual diagnostics are essential for checking model assumptions
          </p>
        </li>
        <li>
          <p>
            Logistic regression is appropriate when the response is binary
          </p>
        </li>
        <li>
          <p>
            Odds ratios provide an intuitive way to interpret logistic regression coefficients
          </p>
        </li>
      </ul>
    </subsection>
    
    <subsection xml:id="subsec-ch09-terms">
      <title>Terms</title>
      
      <p>
        Adjusted R-squared, Backward elimination, Forward selection, Indicator variable, Logistic
        function, Logistic regression, Log-odds, Multiple regression, Odds ratio, Reference level,
        Residual diagnostics, Stepwise selection, Variable selection
      </p>
    </subsection>
  </section>
</chapter>
