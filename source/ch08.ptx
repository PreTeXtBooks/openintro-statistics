<?xml version="1.0" encoding="UTF-8" ?>
<chapter xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="ch-simple-linear-regression">
  <title>Introduction to Linear Regression</title>
  
  <introduction>
    <p>
      Linear regression is a very powerful statistical technique. Many people have some familiarity with regression just from reading the news, where straight lines are overlaid on scatterplots. Linear models can be used for prediction or to evaluate whether there is a linear relationship between two numerical variables.
    </p>
  </introduction>

  <!-- Section 8.1: Fitting a line, residuals, and correlation -->
  <section xml:id="sec-fitting-line-residuals-correlation">
    <title>Fitting a line, residuals, and correlation</title>
    
    <introduction>
      <p>
        It's helpful to think deeply about the line fitting process. In this section, we define the form of a linear model, explore criteria for what makes a good fit, and introduce a new statistic called <term>correlation</term>.
      </p>
    </introduction>

    <subsection xml:id="subsec-fitting-line-to-data">
      <title>Fitting a line to data</title>
      
      <p>
        It is rare for all of the data to fall perfectly on a straight line. Instead, it's more common for data to appear as a <term>cloud of points</term>, where the data fall around a straight line, even if none of the observations fall exactly on the line. In cases where there is a clear relationship between the variables, we will have some uncertainty regarding our estimates of the model parameters.
      </p>

      <definition xml:id="def-linear-model">
        <title>Linear Model</title>
        <statement>
          <p>
            Linear regression is the statistical method for fitting a line to data where the relationship between two variables, <m>x</m> and <m>y</m>, can be modeled by a straight line with some error:
            <me>y = \beta_0 + \beta_1 x + \varepsilon</me>
            where:
            <ul>
              <li><m>\beta_0</m> and <m>\beta_1</m> are the model's <term>parameters</term></li>
              <li><m>\varepsilon</m> represents the error (epsilon)</li>
              <li><m>x</m> is the <term>explanatory variable</term> or <term>predictor</term></li>
              <li><m>y</m> is the <term>response variable</term></li>
            </ul>
            The parameters are estimated using data, and we write their point estimates as <m>b_0</m> and <m>b_1</m>. The fitted line is written as:
            <me>\hat{y} = b_0 + b_1 x</me>
          </p>
        </statement>
      </definition>

      <note>
        <p>
          There are also cases where fitting a straight line to the data, even if there is a clear relationship between the variables, is not helpful. One such case is when the relationship is <term>nonlinear</term>, such as a curved relationship.
        </p>
      </note>
    </subsection>

    <subsection xml:id="subsec-residuals">
      <title>Residuals</title>
      
      <p>
        Residuals are the leftovers from the model fit: the differences between the observed values and the values predicted by the line.
      </p>

      <definition xml:id="def-residual">
        <title>Residual</title>
        <statement>
          <p>
            The <term>residual</term> of the <m>i^{\text{th}}</m> observation is the difference between the observed (<m>y_i</m>) and predicted (<m>\hat{y}_i</m>) response values:
            <me>e_i = y_i - \hat{y}_i</me>
            If an observation lands above the regression line, then its residual, the vertical distance from the observation to the line, is positive. Observations below the line have negative residuals.
          </p>
        </statement>
      </definition>

      <assemblage xml:id="assem-residual-plot">
        <title>Residual Plot</title>
        <p>
          A <term>residual plot</term> is a scatterplot of the residuals (<m>e_i</m>) on the vertical axis against the predictor variable (<m>x_i</m>) on the horizontal axis. These plots are helpful in evaluating whether a linear model is appropriate.
        </p>
        <p>
          Key features to look for:
          <ul>
            <li><em>Random scatter:</em> Residuals should be randomly scattered around zero with no apparent pattern</li>
            <li><em>Constant variance:</em> The variability of residuals should be roughly constant across all values of <m>x</m></li>
            <li><em>No curvature:</em> A curved pattern suggests a nonlinear relationship</li>
          </ul>
        </p>
      </assemblage>
    </subsection>

    <subsection xml:id="subsec-correlation">
      <title>Correlation</title>
      
      <p>
        Correlation measures the strength of the linear relationship between two numerical variables.
      </p>

      <definition xml:id="def-correlation">
        <title>Correlation</title>
        <statement>
          <p>
            The <term>correlation coefficient</term>, denoted by <m>r</m>, describes the strength and direction of the linear relationship between two numerical variables. The correlation is calculated as:
            <me>r = \frac{1}{n-1}\sum_{i=1}^{n} \frac{x_i - \bar{x}}{s_x} \cdot \frac{y_i - \bar{y}}{s_y}</me>
            where <m>s_x</m> and <m>s_y</m> are the sample standard deviations of <m>x</m> and <m>y</m>, respectively.
          </p>
          <p>
            Properties of correlation:
            <ul>
              <li>The correlation is always between -1 and 1: <m>-1 \leq r \leq 1</m></li>
              <li><m>r = -1</m> indicates a perfect negative linear relationship</li>
              <li><m>r = 0</m> indicates no linear relationship</li>
              <li><m>r = 1</m> indicates a perfect positive linear relationship</li>
              <li>Correlation does not imply causation</li>
              <li>Correlation is sensitive to outliers</li>
              <li>Correlation measures only linear relationships</li>
            </ul>
          </p>
        </statement>
      </definition>
    </subsection>
  </section>

  <!-- Section 8.2: Least squares regression -->
  <section xml:id="sec-least-squares-regression">
    <title>Least squares regression</title>
    
    <introduction>
      <p>
        We now introduce a method for finding the coefficients <m>b_0</m> and <m>b_1</m> that minimize the sum of the squared residuals. This approach is called the <term>method of least squares</term>.
      </p>
    </introduction>

    <subsection xml:id="subsec-least-squares-criterion">
      <title>The least squares criterion</title>
      
      <p>
        The <term>least squares line</term> is the line that minimizes the sum of the squared residuals.
      </p>

      <definition xml:id="def-least-squares">
        <title>Least Squares Regression Line</title>
        <statement>
          <p>
            The <term>least squares regression line</term> is the line <m>\hat{y} = b_0 + b_1 x</m> that minimizes:
            <me>\sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2</me>
            The coefficients that accomplish this are:
            <md>
              <mrow>b_1 \amp = r \frac{s_y}{s_x}</mrow>
              <mrow>b_0 \amp = \bar{y} - b_1 \bar{x}</mrow>
            </md>
            where <m>r</m> is the correlation, <m>s_x</m> and <m>s_y</m> are the sample standard deviations, and <m>\bar{x}</m> and <m>\bar{y}</m> are the sample means of <m>x</m> and <m>y</m>, respectively.
          </p>
        </statement>
      </definition>

      <note>
        <title>Important Property</title>
        <p>
          The least squares regression line always passes through the point <m>(\bar{x}, \bar{y})</m>, the point of averages.
        </p>
      </note>
    </subsection>

    <subsection xml:id="subsec-interpreting-regression-coefficients">
      <title>Interpreting regression coefficients</title>
      
      <assemblage xml:id="assem-interpret-slope">
        <title>Interpreting the Slope</title>
        <p>
          The slope <m>b_1</m> describes the estimated change in the response variable <m>y</m> for a one-unit increase in the predictor variable <m>x</m>.
        </p>
        <p>
          <em>Template:</em> For each additional [unit of <m>x</m>], the model predicts an additional (or a decrease of) <m>b_1</m> [units of <m>y</m>], on average.
        </p>
      </assemblage>

      <assemblage xml:id="assem-interpret-intercept">
        <title>Interpreting the Intercept</title>
        <p>
          The intercept <m>b_0</m> describes the estimated value of <m>y</m> when <m>x = 0</m>.
        </p>
        <p>
          <em>Caution:</em> The intercept is often not meaningful if <m>x = 0</m> is outside the range of the observed data. In such cases, the intercept serves mainly as an adjustment constant.
        </p>
      </assemblage>
    </subsection>

    <subsection xml:id="subsec-r-squared">
      <title>Coefficient of determination: <m>R^2</m></title>
      
      <definition xml:id="def-r-squared">
        <title><m>R^2</m> (R-squared)</title>
        <statement>
          <p>
            The <term>coefficient of determination</term>, denoted <m>R^2</m>, describes the proportion of variation in the response variable <m>y</m> that is explained by the least squares line with predictor <m>x</m>:
            <me>R^2 = r^2</me>
            where <m>r</m> is the correlation between <m>x</m> and <m>y</m>.
          </p>
          <p>
            Interpretation: <m>R^2</m> represents the percentage of the variability in <m>y</m> that can be explained by the linear relationship with <m>x</m>. Values range from 0 to 1 (or 0% to 100%).
          </p>
        </statement>
      </definition>
    </subsection>

    <subsection xml:id="subsec-regression-conditions">
      <title>Conditions for least squares regression</title>
      
      <assemblage xml:id="assem-regression-conditions">
        <title>Conditions for Least Squares Regression</title>
        <p>
          For inference in regression, we check the following conditions:
          <ol>
            <li><p><em>Linearity:</em> The relationship between <m>x</m> and <m>y</m> should be linear. Check this with a scatterplot of the data and/or a residual plot.</p></li>
            <li><p><em>Nearly normal residuals:</em> The residuals should be nearly normally distributed. Check this with a histogram or normal probability plot of residuals.</p></li>
            <li><p><em>Constant variability:</em> The variability of points around the least squares line should be roughly constant. Check this with a residual plot.</p></li>
            <li><p><em>Independent observations:</em> The observations should be independent. Consider how the data were collected.</p></li>
          </ol>
        </p>
      </assemblage>
    </subsection>
  </section>

  <!-- Section 8.3: Outliers in linear regression -->
  <section xml:id="sec-outliers-in-regression">
    <title>Types of outliers in linear regression</title>
    
    <p>
      In linear regression, we need to be aware of unusual observations that may influence the regression line or our interpretation of it.
    </p>

    <definition xml:id="def-outlier-in-regression">
      <title>Outlier</title>
      <statement>
        <p>
          An <term>outlier</term> in regression is a point that does not fit the overall pattern of the data. It has a large residual.
        </p>
      </statement>
    </definition>

    <definition xml:id="def-leverage">
      <title>High Leverage Point</title>
      <statement>
        <p>
          A point is said to have <term>high leverage</term> if it is extreme in the <m>x</m> direction. Points with high leverage have the potential to influence the slope of the regression line.
        </p>
      </statement>
    </definition>

    <definition xml:id="def-influential-point">
      <title>Influential Point</title>
      <statement>
        <p>
          An <term>influential point</term> is a point that, if removed, would substantially change the slope of the regression line. Influential points typically have both high leverage and are outliers.
        </p>
      </statement>
    </definition>

    <note>
      <title>Dealing with Unusual Observations</title>
      <p>
        When encountering unusual observations:
        <ul>
          <li>Investigate whether there is an error in data entry or measurement</li>
          <li>Consider whether the observation belongs to a different population</li>
          <li>Assess the impact on the regression by fitting the model with and without the point</li>
          <li>Report findings both with and without influential points when appropriate</li>
        </ul>
      </p>
    </note>
  </section>

  <!-- Section 8.4: Inference for linear regression -->
  <section xml:id="sec-inference-for-regression">
    <title>Inference for linear regression</title>
    
    <introduction>
      <p>
        Just as we performed inference for means and proportions, we can perform inference for the slope and intercept of a regression line.
      </p>
    </introduction>

    <subsection xml:id="subsec-understanding-regression-model">
      <title>Understanding the regression model</title>
      
      <p>
        When we fit a regression line to data, we are estimating the true population regression line:
        <me>y = \beta_0 + \beta_1 x + \varepsilon</me>
        where <m>\varepsilon \sim N(0, \sigma)</m>. Our estimates <m>b_0</m> and <m>b_1</m> are point estimates of the true parameters <m>\beta_0</m> and <m>\beta_1</m>.
      </p>
    </subsection>

    <subsection xml:id="subsec-se-of-slope">
      <title>Standard error of the slope</title>
      
      <definition xml:id="def-se-slope">
        <title>Standard Error of the Slope</title>
        <statement>
          <p>
            The <term>standard error of the slope</term> <m>b_1</m> is given by:
            <me>SE_{b_1} = \frac{s}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}}</me>
            where <m>s</m> is the <term>residual standard error</term>:
            <me>s = \sqrt{\frac{\sum_{i=1}^{n} e_i^2}{n-2}} = \sqrt{\frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{n-2}}</me>
          </p>
        </statement>
      </definition>
    </subsection>

    <subsection xml:id="subsec-hypothesis-test-for-slope">
      <title>Hypothesis test for the slope</title>
      
      <assemblage xml:id="assem-ht-for-slope">
        <title>Hypothesis Test for <m>\beta_1</m></title>
        <p>
          To test whether there is a linear relationship between <m>x</m> and <m>y</m>:
          <ul>
            <li><m>H_0: \beta_1 = 0</m> (no linear relationship)</li>
            <li><m>H_A: \beta_1 \neq 0</m> (linear relationship exists)</li>
          </ul>
          Test statistic:
          <me>t = \frac{b_1 - 0}{SE_{b_1}}</me>
          with degrees of freedom <m>df = n - 2</m>, where <m>n</m> is the number of observations.
        </p>
      </assemblage>
    </subsection>

    <subsection xml:id="subsec-ci-for-slope">
      <title>Confidence interval for the slope</title>
      
      <assemblage xml:id="assem-ci-for-slope">
        <title>Confidence Interval for <m>\beta_1</m></title>
        <p>
          A confidence interval for the true slope <m>\beta_1</m> can be constructed as:
          <me>b_1 \pm t^*_{df} \times SE_{b_1}</me>
          where <m>t^*_{df}</m> is the critical value from the <m>t</m>-distribution with <m>df = n - 2</m> degrees of freedom.
        </p>
      </assemblage>
    </subsection>

    <subsection xml:id="subsec-prediction-intervals">
      <title>Prediction intervals</title>
      
      <p>
        A confidence interval for the average response differs from a prediction interval for an individual response.
      </p>

      <assemblage xml:id="assem-prediction-interval">
        <title>Prediction Interval vs Confidence Interval</title>
        <p>
          <ul>
            <li><p><em>Confidence interval for the mean response:</em> An interval estimate for the average value of <m>y</m> at a particular value <m>x^*</m>. This interval is narrower.</p></li>
            <li><p><em>Prediction interval for an individual response:</em> An interval estimate for a single new observation's value of <m>y</m> at <m>x^*</m>. This interval is wider because it accounts for both the uncertainty in estimating the mean and the variability of individual observations.</p></li>
          </ul>
        </p>
      </assemblage>
    </subsection>
  </section>

  <!-- Section 8.5: Review exercises -->
  <section xml:id="sec-ch08-review">
    <title>Chapter 8 Review</title>
    
    <introduction>
      <p>
        This chapter introduced simple linear regression, a powerful tool for modeling the relationship between two numerical variables. Key concepts include:
      </p>
      <p>
        <ul>
          <li>The linear model: <m>y = \beta_0 + \beta_1 x + \varepsilon</m></li>
          <li>Residuals and residual plots for assessing model fit</li>
          <li>Correlation as a measure of linear association</li>
          <li>Least squares method for finding the best-fitting line</li>
          <li>Coefficient of determination (<m>R^2</m>) for quantifying model performance</li>
          <li>Conditions for regression: linearity, normal residuals, constant variance, independence</li>
          <li>Outliers, leverage points, and influential observations</li>
          <li>Inference for the slope: hypothesis tests and confidence intervals</li>
          <li>Prediction intervals for future observations</li>
        </ul>
      </p>
    </introduction>

    <exercises>
      <title>Practice Exercises</title>
      
      <exercise>
        <statement>
          <p>
            Describe what the correlation coefficient measures and list three important properties of correlation.
          </p>
        </statement>
      </exercise>

      <exercise>
        <statement>
          <p>
            Explain the difference between an outlier, a high leverage point, and an influential point in regression analysis.
          </p>
        </statement>
      </exercise>

      <exercise>
        <statement>
          <p>
            What are the four conditions that should be checked before performing inference in linear regression?
          </p>
        </statement>
      </exercise>

      <exercise>
        <statement>
          <p>
            If the correlation between two variables is <m>r = 0.8</m>, what proportion of the variation in <m>y</m> is explained by the linear relationship with <m>x</m>?
          </p>
        </statement>
      </exercise>
    </exercises>
  </section>

</chapter>
