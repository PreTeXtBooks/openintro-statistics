<?xml version="1.0" encoding="UTF-8"?>
<chapter xml:id="ch-inference-for-means" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Inference for Numerical Data</title>
  
  <introduction>
    <p>
      Chapter<nbsp/><xref ref="ch-foundations-for-inference"/> introduced a framework for 
      statistical inference based on confidence intervals and hypotheses using the normal 
      distribution for sample proportions. In this chapter, we encounter several new point 
      estimates and a couple new distributions. In each case, the inference ideas remain the 
      same: determine which point estimate or test statistic is useful, identify an appropriate 
      distribution for the point estimate or test statistic, and apply the ideas of inference.
    </p>
  </introduction>
  
  <!-- Section 6.1: One-sample means with the t-distribution -->
  <!-- Section 7.1: One-sample means with the t-distribution -->
  <section xml:id="oneSampleMeansWithTDistribution">
    <title>One-sample means with the <m>t</m>-distribution</title>
    
    <introduction>
      <p>
        Similar to how we can model the behavior of the sample proportion <m>\hat{p}</m> using
        a normal distribution, the sample mean <m>\bar{x}</m> can also be modeled using a normal
        distribution when certain conditions are met. However, we'll soon learn that a new
        distribution, called the <m>t</m>-distribution, tends to be more useful when working with
        the sample mean. We'll first learn about this new distribution, then we'll use it to
        construct confidence intervals and conduct hypothesis tests for the mean.
      </p>
    </introduction>
    
    <subsection xml:id="x_bar_sampling_distribution">
      <title>The sampling distribution of <m>\bar{x}</m></title>
      
      <p>
        The sample mean tends to follow a normal distribution centered at the population mean, <m>\mu</m>,
        when certain conditions are met. Additionally, we can compute a standard error for the sample
        mean using the population standard deviation <m>\sigma</m> and the sample size <m>n</m>.
      </p>
      
      <theorem xml:id="clt-mean">
        <title>Central Limit Theorem for the sample mean</title>
        <statement>
          <p>
            When we collect a sufficiently large sample of <m>n</m> independent observations from a
            population with mean <m>\mu</m> and standard deviation <m>\sigma</m>, the sampling
            distribution of <m>\bar{x}</m> will be nearly normal with
          </p>
          <md>
            <mrow>\text{Mean}=\mu \amp\amp \text{Standard Error }(SE) = \frac{\sigma}{\sqrt{n}}</mrow>
          </md>
        </statement>
      </theorem>
      
      <p>
        Before diving into confidence intervals and hypothesis tests using <m>\bar{x}</m>, we first
        need to cover two topics:
      </p>
      
      <ul>
        <li>
          <p>
            When we modeled <m>\hat{p}</m> using the normal distribution, certain conditions had to be
            satisfied. The conditions for working with <m>\bar{x}</m> are a little more complex, and
            we'll spend Section<nbsp/><xref ref="x_bar_conditions"/> discussing how to check conditions
            for inference.
          </p>
        </li>
        <li>
          <p>
            The standard error is dependent on the population standard deviation, <m>\sigma</m>. However,
            we rarely know <m>\sigma</m>, and instead we must estimate it. Because this estimation is
            itself imperfect, we use a new distribution called the <m>t</m>-distribution to fix this
            problem, which we discuss in Section<nbsp/><xref ref="introducingTheTDistribution"/>.
          </p>
        </li>
      </ul>
    </subsection>
    
    <subsection xml:id="x_bar_conditions">
      <title>Evaluating the two conditions required for modeling <m>\bar{x}</m></title>
      
      <p>
        Two conditions are required to apply the Central Limit Theorem for a sample mean <m>\bar{x}</m>:
      </p>
      
      <dl>
        <li>
          <title>Independence.</title>
          <p>
            The sample observations must be independent. The most common way to satisfy this condition
            is when the sample is a simple random sample from the population. If the data come from a
            random process, analogous to rolling a die, this would also satisfy the independence condition.
          </p>
        </li>
        <li>
          <title>Normality.</title>
          <p>
            When a sample is small, we also require that the sample observations come from a normally
            distributed population. We can relax this condition more and more for larger and larger
            sample sizes. This condition is obviously vague, making it difficult to evaluate, so next
            we introduce a couple rules of thumb to make checking this condition easier.
          </p>
        </li>
      </dl>
      
      <assemblage xml:id="normality-rules-thumb">
        <title>Rules of thumb: how to perform the normality check</title>
        <p>
          There is no perfect way to check the normality condition, so instead we use two rules of thumb:
        </p>
        <dl>
          <li>
            <title><m>n \lt 30</m>:</title>
            <p>
              If the sample size <m>n</m> is less than 30 and there are no clear outliers in the data,
              then we typically assume the data come from a nearly normal distribution to satisfy the
              condition.
            </p>
          </li>
          <li>
            <title><m>n \geq 30</m>:</title>
            <p>
              If the sample size <m>n</m> is at least 30 and there are no <em>particularly extreme</em>
              outliers, then we typically assume the sampling distribution of <m>\bar{x}</m> is nearly
              normal, even if the underlying distribution of individual observations is not.
            </p>
          </li>
        </dl>
      </assemblage>
      
      <p>
        In this first course in statistics, you aren't expected to develop perfect judgement on the
        normality condition. However, you are expected to be able to handle clear cut cases based on
        the rules of thumb.<fn>More nuanced guidelines would consider further relaxing the
        <em>particularly extreme outlier</em> check when the sample size is very large. However, we'll
        leave further discussion here to a future course.</fn>
      </p>
      
      <example xml:id="outliers_and_ss_condition_ex">
        <statement>
          <p>
            Consider the following two plots that come from simple random samples from different
            populations. Their sample sizes are <m>n_1 = 15</m> and <m>n_2 = 50</m>.
          </p>
          <p>
            [Figure showing two histograms: Sample 1 Observations (n=15) with values 0-7, and Sample 2
            Observations (n=50) with values 0-22 with most data near zero and one outlier at 21-22]
          </p>
          <p>
            Are the independence and normality conditions met in each case?
          </p>
        </statement>
        <solution>
          <p>
            Each sample is from a simple random sample of its respective population, so the independence
            condition is satisfied. Let's next check the normality condition for each using the rule of thumb.
          </p>
          <p>
            The first sample has fewer than 30 observations, so we are watching for any clear outliers.
            None are present; while there is a small gap in the histogram between 5 and 6, this gap is
            small and 20% of the observations in this small sample are represented in that far right bar
            of the histogram, so we can hardly call these clear outliers. With no clear outliers, the
            normality condition is reasonably met.
          </p>
          <p>
            The second sample has a sample size greater than 30 and includes an outlier that appears to
            be roughly 5 times further from the center of the distribution than the next furthest
            observation. This is an example of a particularly extreme outlier, so the normality condition
            would not be satisfied.
          </p>
        </solution>
      </example>
      
      <p>
        In practice, it's typical to also do a mental check to evaluate whether we have reason to believe
        the underlying population would have moderate skew (if <m>n \lt 30</m>) or have particularly
        extreme outliers (<m>n \geq 30</m>) beyond what we observe in the data. For example, consider
        the number of followers for each individual account on Twitter, and then imagine this distribution.
        The large majority of accounts have built up a couple thousand followers or fewer, while a
        relatively tiny fraction have amassed tens of millions of followers, meaning the distribution is
        extremely skewed. When we know the data come from such an extremely skewed distribution, it takes
        some effort to understand what sample size is large enough for the normality condition to be
        satisfied.
      </p>
    </subsection>
    
    <subsection xml:id="introducingTheTDistribution">
      <title>Introducing the <m>t</m>-distribution</title>
      
      <p>
        In practice, we cannot directly calculate the standard error for <m>\bar{x}</m> since we do not
        know the population standard deviation, <m>\sigma</m>. We encountered a similar issue when
        computing the standard error for a sample proportion, which relied on the population proportion,
        <m>p</m>. Our solution in the proportion context was to use the sample value in place of the
        population value when computing the standard error. We'll employ a similar strategy for computing
        the standard error of <m>\bar{x}</m>, using the sample standard deviation <m>s</m> in place of
        <m>\sigma</m>:
      </p>
      <me>
        SE = \frac{\sigma}{\sqrt{n}} \approx \frac{s}{\sqrt{n}}
      </me>
      <p>
        This strategy tends to work well when we have a lot of data and can estimate <m>\sigma</m> using
        <m>s</m> accurately. However, the estimate is less precise with smaller samples, and this leads
        to problems when using the normal distribution to model <m>\bar{x}</m>.
      </p>
      
      <p>
        We'll find it useful to use a new distribution for inference calculations called the
        <term><m>t</m>-distribution</term>. A <m>t</m>-distribution, shown as a solid line in
        Figure<nbsp/><xref ref="tDistCompareToNormalDist"/>, has a bell shape. However, its tails are
        thicker than the normal distribution's, meaning observations are more likely to fall beyond two
        standard deviations from the mean than under the normal distribution. The extra thick tails of
        the <m>t</m>-distribution are exactly the correction needed to resolve the problem of using
        <m>s</m> in place of <m>\sigma</m> in the <m>SE</m> calculation.
      </p>
      
      <figure xml:id="tDistCompareToNormalDist">
        <caption>Comparison of a <m>t</m>-distribution and a normal distribution.</caption>
        <p>
          [Figure showing a standard normal distribution and a t-distribution overlaid. The t-distribution
          is more sharply peaked and has thicker tails than the normal distribution.]
        </p>
      </figure>
      
      <p>
        The <m>t</m>-distribution is always centered at zero and has a single parameter: degrees of
        freedom. The <term>degrees of freedom (<m>df</m>)</term> describes the precise form of the
        bell-shaped <m>t</m>-distribution. Several <m>t</m>-distributions are shown in
        Figure<nbsp/><xref ref="tDistConvergeToNormalDist"/> in comparison to the normal distribution.
      </p>
      
      <p>
        In general, we'll use a <m>t</m>-distribution with <m>df = n - 1</m> to model the sample mean
        when the sample size is <m>n</m>. That is, when we have more observations, the degrees of freedom
        will be larger and the <m>t</m>-distribution will look more like the standard normal distribution;
        when the degrees of freedom is about 30 or more, the <m>t</m>-distribution is nearly
        indistinguishable from the normal distribution.
      </p>
      
      <figure xml:id="tDistConvergeToNormalDist">
        <caption>The larger the degrees of freedom, the more closely the <m>t</m>-distribution resembles
        the standard normal distribution.</caption>
        <p>
          [Figure showing four t-distributions with df=1, 2, 4, and 8 along with a normal distribution.
          The larger the df, the more closely the t-distribution aligns with the normal distribution.]
        </p>
      </figure>
      
      <assemblage xml:id="degrees-freedom-def">
        <title>Degrees of freedom (<m>df</m>)</title>
        <p>
          The degrees of freedom describes the shape of the <m>t</m>-distribution. The larger the degrees
          of freedom, the more closely the distribution approximates the normal model.
        </p>
        <p>
          When modeling <m>\bar{x}</m> using the <m>t</m>-distribution, use <m>df = n - 1</m>.
        </p>
      </assemblage>
      
      <p>
        The <m>t</m>-distribution allows us greater flexibility than the normal distribution when
        analyzing numerical data. In practice, it's common to use statistical software, such as R, Python,
        or SAS for these analyses. Alternatively, a graphing calculator or a <term><m>t</m>-table</term>
        may be used; the <m>t</m>-table is similar to the normal distribution table, and it may be found
        in the appendix<!-- TODO: Add xref to t-distribution table appendix when available -->, which 
        includes usage instructions and examples for those who wish to use this option. No matter the 
        approach you choose, apply your method using the examples below to confirm your working understanding 
        of the <m>t</m>-distribution.
      </p>
      
      <example xml:id="t-dist-tail-18df">
        <statement>
          <p>
            What proportion of the <m>t</m>-distribution with 18 degrees of freedom falls below <m>-2.10</m>?
          </p>
        </statement>
        <solution>
          <p>
            Just like a normal probability problem, we first draw the picture in
            Figure<nbsp/><xref ref="tDistDF18LeftTail2Point10"/> and shade the area below <m>-2.10</m>.
            Using statistical software, we can obtain a precise value: 0.0250.
          </p>
        </solution>
      </example>
      
      <figure xml:id="tDistDF18LeftTail2Point10">
        <caption>The <m>t</m>-distribution with 18 degrees of freedom. The area below <m>-2.10</m> has
        been shaded.</caption>
        <p>
          [Figure showing a t-distribution with 18 df, with the region below -2.10 shaded, representing
          roughly 2% to 5% of the distribution.]
        </p>
      </figure>
      
      <example xml:id="t-dist-tail-20df">
        <statement>
          <p>
            A <m>t</m>-distribution with 20 degrees of freedom is shown in the left panel of
            Figure<nbsp/><xref ref="tDistDF20RightTail1Point65"/>. Estimate the proportion of the
            distribution falling above 1.65.
          </p>
        </statement>
        <solution>
          <p>
            With a normal distribution, this would correspond to about 0.05, so we should expect the
            <m>t</m>-distribution to give us a value in this neighborhood. Using statistical software:
            0.0573.
          </p>
        </solution>
      </example>
      
      <figure xml:id="tDistDF20RightTail1Point65">
        <caption>Left: The <m>t</m>-distribution with 20 degrees of freedom, with the area above 1.65
        shaded. Right: The <m>t</m>-distribution with 2 degrees of freedom, with the area further than
        3 units from 0 shaded.</caption>
        <p>
          [Figure showing two plots: left shows t-dist with 20 df and right tail shaded above 1.65; right
          shows t-dist with 2 df with both tails beyond Â±3 shaded.]
        </p>
      </figure>
      
      <example xml:id="t-dist-tail-2df">
        <statement>
          <p>
            A <m>t</m>-distribution with 2 degrees of freedom is shown in the right panel of
            Figure<nbsp/><xref ref="tDistDF20RightTail1Point65"/>. Estimate the proportion of the
            distribution falling more than 3 units from the mean (above or below).
          </p>
        </statement>
        <solution>
          <p>
            With so few degrees of freedom, the <m>t</m>-distribution will give a more notably different
            value than the normal distribution. Under a normal distribution, the area would be about 0.003
            using the 68-95-99.7 rule. For a <m>t</m>-distribution with <m>df = 2</m>, the area in both
            tails beyond 3 units totals 0.0955. This area is dramatically different than what we obtain
            from the normal distribution.
          </p>
        </solution>
      </example>
      
      <exercise>
        <statement>
          <p>
            What proportion of the <m>t</m>-distribution with 19 degrees of freedom falls above
            <m>-1.79</m> units? Use your preferred method for finding tail areas.<fn>We want to find the
            shaded area <em>above</em> <m>-1.79</m> (we leave the picture to you). The lower tail area
            has an area of 0.0447, so the upper area would have an area of <m>1 - 0.0447 = 0.9553</m>.</fn>
          </p>
        </statement>
      </exercise>
    </subsection>
    
    <subsection xml:id="oneSampleTConfidenceIntervals">
      <title>One sample <m>t</m>-confidence intervals</title>
      
      <p>
        Let's get our first taste of applying the <m>t</m>-distribution in the context of an example
        about the mercury content of dolphin muscle. Elevated mercury concentrations are an important
        problem for both dolphins and other animals, like humans, who occasionally eat them.
      </p>
      
      <figure xml:id="rissosDolphin">
        <caption>A Risso's dolphin. Photo by Mike Baird (www.bairdphotos.com). CC BY 2.0 license.</caption>
        <p>
          [Figure showing a Risso's dolphin surfacing in water. The area forward of its face is mostly
          white, and then its body is gray and white streaked together.]
        </p>
      </figure>
      
      <p>
        We will identify a confidence interval for the average mercury content in dolphin muscle using a
        sample of 19 Risso's dolphins from the Taiji area in Japan. The data are summarized in
        Figure<nbsp/><xref ref="summaryStatsOfHgInMuscleOfRissosDolphins"/>. The minimum and maximum
        observed values can be used to evaluate whether or not there are clear outliers.
      </p>
      
      <figure xml:id="summaryStatsOfHgInMuscleOfRissosDolphins">
        <caption>Summary of mercury content in the muscle of 19 Risso's dolphins from the Taiji area.
        Measurements are in micrograms of mercury per wet gram of muscle (<m>\mu</m>g/wet g).</caption>
        <tabular>
          <row bottom="medium">
            <cell><m>n</m></cell>
            <cell><m>\bar{x}</m></cell>
            <cell><m>s</m></cell>
            <cell>minimum</cell>
            <cell>maximum</cell>
          </row>
          <row>
            <cell>19</cell>
            <cell>4.4</cell>
            <cell>2.3</cell>
            <cell>1.7</cell>
            <cell>9.2</cell>
          </row>
        </tabular>
      </figure>
      
      <example xml:id="dolphin-conditions">
        <statement>
          <p>
            Are the independence and normality conditions satisfied for this data set?
          </p>
        </statement>
        <solution>
          <p>
            The observations are a simple random sample, therefore independence is reasonable. The summary
            statistics in Figure<nbsp/><xref ref="summaryStatsOfHgInMuscleOfRissosDolphins"/> do not
            suggest any clear outliers, since all observations are within 2.5 standard deviations of the
            mean. Based on this evidence, the normality condition seems reasonable.
          </p>
        </solution>
      </example>
      
      <p>
        In the normal model, we used <m>z^{\star}</m> and the standard error to determine the width of
        a confidence interval. We revise the confidence interval formula slightly when using the
        <m>t</m>-distribution:
      </p>
      <md>
        <mrow>\text{point estimate} \pm t^{\star}_{df} \times SE \amp\quad \to \quad \bar{x} \pm t^{\star}_{df} \times \frac{s}{\sqrt{n}}</mrow>
      </md>
      
      <example xml:id="dolphin-se">
        <statement>
          <p>
            Using the summary statistics in Figure<nbsp/><xref ref="summaryStatsOfHgInMuscleOfRissosDolphins"/>,
            compute the standard error for the average mercury content in the <m>n = 19</m> dolphins.
          </p>
        </statement>
        <solution>
          <p>
            We plug in <m>s</m> and <m>n</m> into the formula: <m>SE = s / \sqrt{n} = 2.3 / \sqrt{19} = 0.528</m>.
          </p>
        </solution>
      </example>
      
      <p>
        The value <m>t^{\star}_{df}</m> is a cutoff we obtain based on the confidence level and the
        <m>t</m>-distribution with <m>df</m> degrees of freedom. That cutoff is found in the same way as
        with a normal distribution: we find <m>t^{\star}_{df}</m> such that the fraction of the
        <m>t</m>-distribution with <m>df</m> degrees of freedom within a distance <m>t^{\star}_{df}</m>
        of 0 matches the confidence level of interest.
      </p>
      
      <example xml:id="dolphin-df-tstar">
        <statement>
          <p>
            When <m>n = 19</m>, what is the appropriate degrees of freedom? Find <m>t^{\star}_{df}</m>
            for this degrees of freedom and the confidence level of 95%.
          </p>
        </statement>
        <solution>
          <p>
            The degrees of freedom is easy to calculate: <m>df = n - 1 = 18</m>.
          </p>
          <p>
            Using statistical software, we find the cutoff where the upper tail is equal to 2.5%:
            <m>t^{\star}_{18} = 2.10</m>. The area below <m>-2.10</m> will also be equal to 2.5%. That
            is, 95% of the <m>t</m>-distribution with <m>df = 18</m> lies within 2.10 units of 0.
          </p>
        </solution>
      </example>
      
      <example xml:id="dolphin-ci">
        <statement>
          <p>
            Compute and interpret the 95% confidence interval for the average mercury content in Risso's
            dolphins.
          </p>
        </statement>
        <solution>
          <p>
            We can construct the confidence interval as
          </p>
          <md>
            <mrow>\bar{x} \pm t^{\star}_{18} \times SE \amp\quad \to \quad 4.4 \pm 2.10 \times 0.528</mrow>
            <mrow>\amp\quad \to \quad (3.29, 5.51)</mrow>
          </md>
          <p>
            We are 95% confident the average mercury content of muscles in Risso's dolphins is between
            3.29 and 5.51 <m>\mu</m>g/wet gram, which is considered extremely high.
          </p>
        </solution>
      </example>
      
      <assemblage xml:id="t-ci-mean">
        <title>Finding a <m>t</m>-confidence interval for the mean</title>
        <p>
          Based on a sample of <m>n</m> independent and nearly normal observations, a confidence interval
          for the population mean is
        </p>
        <md>
          <mrow>\text{point estimate} \pm t^{\star}_{df} \times SE \amp\quad \to \quad \bar{x} \pm t^{\star}_{df} \times \frac{s}{\sqrt{n}}</mrow>
        </md>
        <p>
          where <m>\bar{x}</m> is the sample mean, <m>t^{\star}_{df}</m> corresponds to the confidence
          level and degrees of freedom <m>df</m>, and <m>SE</m> is the standard error as estimated by
          the sample.
        </p>
      </assemblage>
      
      <exercise xml:id="croakerWhiteFishPacificExerConditions">
        <statement>
          <p>
            The FDA's webpage provides some data on mercury content of fish. Based on a sample of 15
            croaker white fish (Pacific), a sample mean and standard deviation were computed as 0.287 and
            0.069 ppm (parts per million), respectively. The 15 observations ranged from 0.18 to 0.41 ppm.
            We will assume these observations are independent. Based on the summary statistics of the data,
            do you have any objections to the normality condition of the individual observations?<fn>The
            sample size is under 30, so we check for obvious outliers: since all observations are within 2
            standard deviations of the mean, there are no such clear outliers.</fn>
          </p>
        </statement>
      </exercise>
      
      <example xml:id="croakerWhiteFishPacificExerSEDFTStar">
        <statement>
          <p>
            Estimate the standard error of <m>\bar{x} = 0.287</m> ppm using the data summaries in
            Guided Practice<nbsp/><xref ref="croakerWhiteFishPacificExerConditions"/>. If we are to use
            the <m>t</m>-distribution to create a 90% confidence interval for the actual mean of the
            mercury content, identify the degrees of freedom and <m>t^{\star}_{df}</m>.
          </p>
        </statement>
        <solution>
          <p>
            The standard error: <m>SE = \frac{0.069}{\sqrt{15}} = 0.0178</m>.
          </p>
          <p>
            Degrees of freedom: <m>df = n - 1 = 14</m>.
          </p>
          <p>
            Since the goal is a 90% confidence interval, we choose <m>t_{14}^{\star}</m> so that the
            two-tail area is 0.1: <m>t^{\star}_{14} = 1.76</m>.
          </p>
        </solution>
      </example>
      
      <assemblage xml:id="ci-one-mean-steps">
        <title>Confidence interval for a single mean</title>
        <p>
          Once you've determined a one-mean confidence interval would be helpful for an application, there
          are four steps to constructing the interval:
        </p>
        <dl>
          <li>
            <title>Prepare.</title>
            <p>
              Identify <m>\bar{x}</m>, <m>s</m>, <m>n</m>, and determine what confidence level you wish
              to use.
            </p>
          </li>
          <li>
            <title>Check.</title>
            <p>
              Verify the conditions to ensure <m>\bar{x}</m> is nearly normal.
            </p>
          </li>
          <li>
            <title>Calculate.</title>
            <p>
              If the conditions hold, compute <m>SE</m>, find <m>t_{df}^{\star}</m>, and construct the
              interval.
            </p>
          </li>
          <li>
            <title>Conclude.</title>
            <p>
              Interpret the confidence interval in the context of the problem.
            </p>
          </li>
        </dl>
      </assemblage>
      
      <exercise xml:id="croakerWhiteFish90ci">
        <statement>
          <p>
            Using the information and results of Guided Practice<nbsp/><xref ref="croakerWhiteFishPacificExerConditions"/>
            and Example<nbsp/><xref ref="croakerWhiteFishPacificExerSEDFTStar"/>, compute a 90% confidence
            interval for the average mercury content of croaker white fish (Pacific).<fn><m>\bar{x} \pm t^{\star}_{14} \times SE \to 0.287 \pm 1.76 \times 0.0178 \to (0.256, 0.318)</m>.
            We are 90% confident that the average mercury content of croaker white fish (Pacific) is
            between 0.256 and 0.318 ppm.</fn>
          </p>
        </statement>
      </exercise>
      
      <exercise>
        <statement>
          <p>
            The 90% confidence interval from Guided Practice<nbsp/><xref ref="croakerWhiteFish90ci"/> is
            0.256 ppm to 0.318 ppm. Can we say that 90% of croaker white fish (Pacific) have mercury levels
            between 0.256 and 0.318 ppm?<fn>No, a confidence interval only provides a range of plausible
            values for a population parameter, in this case the population mean. It does not describe what
            we might observe for individual observations.</fn>
          </p>
        </statement>
      </exercise>
    </subsection>
    
    <subsection xml:id="oneSampleTTests">
      <title>One sample <m>t</m>-tests</title>
      
      <p>
        Is the typical US runner getting faster or slower over time? We consider this question in the
        context of the Cherry Blossom Race, which is a 10-mile race in Washington, DC each spring.
      </p>
      
      <p>
        The average time for all runners who finished the Cherry Blossom Race in 2006 was 93.29 minutes
        (93 minutes and about 17 seconds). We want to determine using data from 100 participants in the
        2017 Cherry Blossom Race whether runners in this race are getting faster or slower, versus the
        other possibility that there has been no change.
      </p>
      
      <exercise>
        <statement>
          <p>
            What are appropriate hypotheses for this context?<fn><m>H_0</m>: The average 10-mile run time
            was the same for 2006 and 2017. <m>\mu = 93.29</m> minutes. <m>H_A</m>: The average 10-mile
            run time for 2017 was <em>different</em> than that of 2006. <m>\mu \neq 93.29</m> minutes.</fn>
          </p>
        </statement>
      </exercise>
      
      <exercise>
        <statement>
          <p>
            The data come from a simple random sample of all participants, so the observations are
            independent. However, should we be worried about the normality condition? See
            Figure<nbsp/><xref ref="run10SampTimeHistogram"/> for a histogram of the differences and
            evaluate if we can move forward.<fn>With a sample of 100, we should only be concerned if there
            are particularly extreme outliers. The histogram of the data doesn't show any outliers of
            concern (and arguably, no outliers at all).</fn>
          </p>
        </statement>
      </exercise>
      
      <figure xml:id="run10SampTimeHistogram">
        <caption>A histogram of time for the sample Cherry Blossom Race data.</caption>
        <p>
          [Figure showing a histogram of "time" for the sample. The data are nearly symmetric with a
          center at about 100 minutes and a standard deviation of roughly 15 to 20 minutes. All times lie
          between 50 and 140 minutes.]
        </p>
      </figure>
      
      <p>
        When completing a hypothesis test for the one-sample mean, the process is nearly identical to
        completing a hypothesis test for a single proportion. First, we find the Z-score using the
        observed value, null value, and standard error; however, we call it a <term>T-score</term> since
        we use a <m>t</m>-distribution for calculating the tail area. Then we find the p-value using the
        same ideas we used previously: find the one-tail area under the sampling distribution, and double
        it.
      </p>
      
      <example xml:id="cherry-blossom-test">
        <statement>
          <p>
            With both the independence and normality conditions satisfied, we can proceed with a hypothesis
            test using the <m>t</m>-distribution. The sample mean and sample standard deviation of the
            sample of 100 runners from the 2017 Cherry Blossom Race are 97.32 and 16.98 minutes,
            respectively. Recall that the sample size is 100 and the average run time in 2006 was 93.29
            minutes. Find the test statistic and p-value. What is your conclusion?
          </p>
        </statement>
        <solution>
          <p>
            To find the test statistic (T-score), we first must determine the standard error:
          </p>
          <me>
            SE = 16.98 / \sqrt{100} = 1.70
          </me>
          <p>
            Now we can compute the <em>T-score</em> using the sample mean (97.32), null value (93.29),
            and <m>SE</m>:
          </p>
          <me>
            T = \frac{97.32 - 93.29}{1.70} = 2.37
          </me>
          <p>
            For <m>df = 100 - 1 = 99</m>, we can determine using statistical software (or a
            <m>t</m>-table) that the one-tail area is 0.01, which we double to get the p-value: 0.02.
          </p>
          <p>
            Because the p-value is smaller than 0.05, we reject the null hypothesis. That is, the data
            provide strong evidence that the average run time for the Cherry Blossom Run in 2017 is
            different than the 2006 average. Since the observed value is above the null value and we have
            rejected the null hypothesis, we would conclude that runners in the race were slower on average
            in 2017 than in 2006.
          </p>
        </solution>
      </example>
      
      <assemblage xml:id="ht-one-mean-steps">
        <title>Hypothesis testing for a single mean</title>
        <p>
          Once you've determined a one-mean hypothesis test is the correct procedure, there are four steps
          to completing the test:
        </p>
        <dl>
          <li>
            <title>Prepare.</title>
            <p>
              Identify the parameter of interest, list out hypotheses, identify the significance level,
              and identify <m>\bar{x}</m>, <m>s</m>, and <m>n</m>.
            </p>
          </li>
          <li>
            <title>Check.</title>
            <p>
              Verify conditions to ensure <m>\bar{x}</m> is nearly normal.
            </p>
          </li>
          <li>
            <title>Calculate.</title>
            <p>
              If the conditions hold, compute <m>SE</m>, compute the T-score, and identify the p-value.
            </p>
          </li>
          <li>
            <title>Conclude.</title>
            <p>
              Evaluate the hypothesis test by comparing the p-value to <m>\alpha</m>, and provide a
              conclusion in the context of the problem.
            </p>
          </li>
        </dl>
      </assemblage>
    </subsection>
    
    <exercises>
      <title>Section Exercises</title>
      
      <exercise xml:id="identify_critical_t">
        <title>Identify the critical <m>t</m></title>
        <statement>
          <p>
            An independent random sample is selected from an approximately normal population with unknown
            standard deviation. Find the degrees of freedom and the critical <m>t</m>-value (<m>t^\star</m>)
            for the given sample size and confidence level.
          </p>
          <ol>
            <li><m>n = 6</m>, CL = 90%</li>
            <li><m>n = 21</m>, CL = 98%</li>
            <li><m>n = 29</m>, CL = 95%</li>
            <li><m>n = 12</m>, CL = 99%</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="t_distribution">
        <title><m>t</m>-distribution</title>
        <statement>
          <p>
            The figure on the right shows three unimodal and symmetric curves: the standard normal (z) 
            distribution, the <m>t</m>-distribution with 5 degrees of freedom, and the 
            <m>t</m>-distribution with 1 degree of freedom. Determine which is which, and explain your 
            reasoning.
          </p>
          <p>
            <!-- TODO: Add figure reference - Three distributions are shown, all symmetric, bell-shaped, 
            and centered at zero. The first is shown as a solid line and has the broadest peak of the three 
            distributions, and the tails of this distribution also visually approach zero at about -3 and 
            positive 3. The second curve that is shown as a dashed line has a less broad, slightly sharper 
            peak than the distribution based on solid line. The tails of the distribution with the dashed 
            line has tails that visually approach zero at values of about -4 and positive 4. The third curve 
            is shown as a dotted line and has the sharpest peak of the three distributions. The tails of the 
            dotted line distribution has tails that visually approach zero further out, beyond the limits 
            shown in this plot of -4 and positive 4. -->
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="find_T_pval_1_2_sided">
        <title>Find the p-value, Part I</title>
        <statement>
          <p>
            An independent random sample is selected from an approximately normal population with an
            unknown standard deviation. Find the p-value for the given sample size and test statistic.
            Also determine if the null hypothesis would be rejected at <m>\alpha = 0.05</m>.
          </p>
          <ol>
            <li><m>n = 11</m>, <m>T = 1.91</m></li>
            <li><m>n = 17</m>, <m>T = -3.45</m></li>
            <li><m>n = 7</m>, <m>T = 0.83</m></li>
            <li><m>n = 28</m>, <m>T = 2.13</m></li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="find_T_pval_2_2_sided">
        <title>Find the p-value, Part II</title>
        <statement>
          <p>
            An independent random sample is selected from an approximately normal population with an
            unknown standard deviation. Find the p-value for the given sample size and test statistic.
            Also determine if the null hypothesis would be rejected at <m>\alpha = 0.01</m>.
          </p>
          <ol>
            <li><m>n = 26</m>, <m>T = 2.485</m></li>
            <li><m>n = 18</m>, <m>T = 0.5</m></li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="work_backwards_1">
        <title>Working backwards, Part I</title>
        <statement>
          <p>
            A 95% confidence interval for a population mean, <m>\mu</m>, is given as (18.985, 21.015).
            This confidence interval is based on a simple random sample of 36 observations. Calculate the
            sample mean and standard deviation. Assume that all conditions necessary for inference are
            satisfied. Use the <m>t</m>-distribution in any calculations.
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="work_backwards_2">
        <title>Working backwards, Part II</title>
        <statement>
          <p>
            A 90% confidence interval for a population mean is (65, 77). The population distribution is
            approximately normal and the population standard deviation is unknown. This confidence interval
            is based on a simple random sample of 25 observations. Calculate the sample mean, the margin
            of error, and the sample standard deviation.
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="ny_sleep_habits_2_sided">
        <title>Sleep habits of New Yorkers</title>
        <statement>
          <p>
            New York is known as "the city that never sleeps". A random sample of 25 New Yorkers were
            asked how much sleep they get per night. Statistical summaries of these data are shown below.
            The point estimate suggests New Yorkers sleep less than 8 hours a night on average. Is the
            result statistically significant?
          </p>
          <tabular>
            <row bottom="medium">
              <cell>n</cell>
              <cell><m>\bar{x}</m></cell>
              <cell>s</cell>
              <cell>min</cell>
              <cell>max</cell>
            </row>
            <row>
              <cell>25</cell>
              <cell>7.73</cell>
              <cell>0.77</cell>
              <cell>6.17</cell>
              <cell>9.78</cell>
            </row>
          </tabular>
          <ol>
            <li>Write the hypotheses in symbols and in words.</li>
            <li>Check conditions, then calculate the test statistic, <m>T</m>, and the associated degrees
            of freedom.</li>
            <li>Find and interpret the p-value in this context. Drawing a picture may be helpful.</li>
            <li>What is the conclusion of the hypothesis test?</li>
            <li>If you were to construct a 90% confidence interval that corresponded to this hypothesis
            test, would you expect 8 hours to be in the interval?</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="adult_heights">
        <title>Heights of adults</title>
        <statement>
          <p>
            Researchers studying anthropometry collected body girth measurements and skeletal diameter
            measurements, as well as age, weight, height and gender, for 507 physically active individuals.
            The histogram below shows the sample distribution of heights in centimeters.
          </p>
          <p>
            <!-- TODO: Add figure - A histogram is shown for "Height" with values ranging from 140 to 200, 
            with a bin width of 5. The distribution is roughly symmetric with a center at about 170. The bin 
            heights, starting with the bin from 145 to 150, are about 3, 17, 55, 70, 100, 85, 95, 50, 30, 15, 
            and 3. -->
          </p>
          <p>
            Summary statistics:
          </p>
          <!-- TODO: Convert to proper PreTeXt table -->
          <tabular>
            <row><cell>Min</cell><cell>147.2</cell></row>
            <row><cell>Q1</cell><cell>163.8</cell></row>
            <row><cell>Median</cell><cell>170.3</cell></row>
            <row><cell>Mean</cell><cell>171.1</cell></row>
            <row><cell>SD</cell><cell>9.4</cell></row>
            <row><cell>Q3</cell><cell>177.8</cell></row>
            <row><cell>Max</cell><cell>198.1</cell></row>
          </tabular>
          <ol>
            <li>What is the point estimate for the average height of active individuals? What about the
            median?</li>
            <li>What is the point estimate for the standard deviation of the heights of active individuals?
            What about the IQR?</li>
            <li>Is a person who is 1m 80cm (180 cm) tall considered unusually tall? And is a person who
            is 1m 55cm (155cm) considered unusually short? Explain your reasoning.</li>
            <li>The researchers take another random sample of physically active individuals. Would you
            expect the mean and the standard deviation of this new sample to be the ones given above?
            Explain your reasoning.</li>
            <li>The sample means obtained are point estimates for the mean height of all active
            individuals, if the sample of individuals is equivalent to a simple random sample. What measure
            do we use to quantify the variability of such an estimate? Compute this quantity using the
            data from the original sample under the condition that the data are a simple random sample.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="find_mean_2_sided">
        <title>Find the mean</title>
        <statement>
          <p>
            You are given the following hypotheses:
          </p>
          <md>
            <mrow>H_0\amp: \mu = 60</mrow>
            <mrow>H_A\amp: \mu \neq 60</mrow>
          </md>
          <p>
            We know that the sample standard deviation is 8 and the sample size is 20. For what sample
            mean would the p-value be equal to 0.05? Assume that all conditions necessary for inference
            are satisfied.
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="critical_t_vs_z">
        <title><m>t^\star</m> vs. <m>z^\star</m></title>
        <statement>
          <p>
            For a given confidence level, <m>t^{\star}_{df}</m> is larger than <m>z^{\star}</m>. Explain
            how <m>t^{*}_{df}</m> being slightly larger than <m>z^{*}</m> affects the width of the
            confidence interval.
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="play_piano_2_sided">
        <title>Play the piano</title>
        <statement>
          <p>
            Georgianna claims that in a small city renowned for its music school, the average child takes
            less than 5 years of piano lessons. We have a random sample of 20 children from the city, with
            a mean of 4.6 years of piano lessons and a standard deviation of 2.2 years.
          </p>
          <ol>
            <li>Evaluate Georgianna's claim (or that the opposite might be true) using a hypothesis test.</li>
            <li>Construct a 95% confidence interval for the number of years students in this city take
            piano lessons, and interpret it in context of the data.</li>
            <li>Do your results from the hypothesis test and the confidence interval agree? Explain your
            reasoning.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="auto_exhaust_lead_exposure_2_sided">
        <title>Auto exhaust and lead exposure</title>
        <statement>
          <p>
            Researchers interested in lead exposure due to car exhaust sampled the blood of 52 police
            officers subjected to constant inhalation of automobile exhaust fumes while working traffic
            enforcement in a primarily urban environment. The blood samples of these officers had an
            average lead concentration of 124.32 <m>\mu</m>g/l and a SD of 37.74 <m>\mu</m>g/l; a
            previous study of individuals from a nearby suburb, with no history of exposure, found an
            average blood level concentration of 35 <m>\mu</m>g/l.
          </p>
          <ol>
            <li>Write down the hypotheses that would be appropriate for testing if the police officers
            appear to have been exposed to a different concentration of lead.</li>
            <li>Explicitly state and check all conditions necessary for inference on these data.</li>
            <li>Regardless of your answers in part (b), test the hypothesis that the downtown police
            officers have a higher lead exposure than the group in the previous study. Interpret your
            results in context.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="car_insurance_savings">
        <title>Car insurance savings</title>
        <statement>
          <p>
            A market researcher wants to evaluate car insurance savings at a competing company. Based on
            past studies he is assuming that the standard deviation of savings is $100. He wants to collect
            data such that he can get a margin of error of no more than $10 at a 95% confidence level. How
            large of a sample should he collect?
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="sat_scores_CI">
        <title>SAT scores</title>
        <statement>
          <p>
            The standard deviation of SAT scores for students at a particular Ivy League college is 250
            points. Two statistics students, Raina and Luke, want to estimate the average SAT score of
            students at this college as part of a class project. They want their margin of error to be no
            more than 25 points.
          </p>
          <ol>
            <li>Raina wants to use a 90% confidence interval. How large a sample should she collect?</li>
            <li>Luke wants to use a 99% confidence interval. Without calculating the actual sample size,
            determine whether his sample should be larger or smaller than Raina's, and explain your
            reasoning.</li>
            <li>Calculate the minimum required sample size for Luke.</li>
          </ol>
        </statement>
      </exercise>
    </exercises>
  </section>
  
  <!-- Section 6.2: Paired data -->
  <!-- Section 7.2: Paired data -->
  <section xml:id="pairedData">
    <title>Paired data</title>
    
    <introduction>
      <p>
        In an earlier edition of this textbook, we found that Amazon prices were, on average, lower
        than those of the UCLA Bookstore for UCLA courses in 2010. It's been several years, and many
        stores have adapted to the online market, so we wondered, how is the UCLA Bookstore doing today?
      </p>
      
      <p>
        We sampled 201 UCLA courses. Of those, 68 required books could be found on Amazon. A portion
        of the data set from these courses is shown in Figure<nbsp/><xref ref="textbooksDF"/>, where
        prices are in US dollars.
      </p>
      
      <figure xml:id="textbooksDF">
        <caption>Four cases of the textbooks data set.</caption>
        <tabular>
          <row bottom="medium">
            <cell></cell>
            <cell>subject</cell>
            <cell>course number</cell>
            <cell>bookstore</cell>
            <cell>amazon</cell>
            <cell>price difference</cell>
          </row>
          <row>
            <cell>1</cell>
            <cell>American Indian Studies</cell>
            <cell>M10</cell>
            <cell>47.97</cell>
            <cell>47.45</cell>
            <cell>0.52</cell>
          </row>
          <row>
            <cell>2</cell>
            <cell>Anthropology</cell>
            <cell>2</cell>
            <cell>14.26</cell>
            <cell>13.55</cell>
            <cell>0.71</cell>
          </row>
          <row>
            <cell>3</cell>
            <cell>Arts and Architecture</cell>
            <cell>10</cell>
            <cell>13.50</cell>
            <cell>12.53</cell>
            <cell>0.97</cell>
          </row>
          <row>
            <cell><m>\vdots</m></cell>
            <cell><m>\vdots</m></cell>
            <cell><m>\vdots</m></cell>
            <cell><m>\vdots</m></cell>
            <cell><m>\vdots</m></cell>
            <cell><m>\vdots</m></cell>
          </row>
          <row>
            <cell>68</cell>
            <cell>Jewish Studies</cell>
            <cell>M10</cell>
            <cell>35.96</cell>
            <cell>32.40</cell>
            <cell>3.56</cell>
          </row>
        </tabular>
      </figure>
    </introduction>
    
    <subsection xml:id="paired-observations">
      <title>Paired observations</title>
      
      <p>
        Each textbook has two corresponding prices in the data set: one for the UCLA Bookstore and
        one for Amazon. When two sets of observations have this special correspondence, they are said
        to be <term>paired</term>.
      </p>
      
      <assemblage xml:id="paired-data-def">
        <title>Paired data</title>
        <p>
          Two sets of observations are <em>paired</em> if each observation in one set has a special
          correspondence or connection with exactly one observation in the other data set.
        </p>
      </assemblage>
      
      <p>
        To analyze paired data, it is often useful to look at the difference in outcomes of each pair
        of observations. In the textbook data, we look at the differences in prices, which is
        represented as the <c>price_difference</c> variable in the data set. Here the differences are
        taken as
      </p>
      <me>
        \text{UCLA Bookstore price} - \text{Amazon price}
      </me>
      <p>
        It is important that we always subtract using a consistent order; here Amazon prices are always
        subtracted from UCLA prices. The first difference shown in Figure<nbsp/><xref ref="textbooksDF"/>
        is computed as <m>47.97 - 47.45 = 0.52</m>. Similarly, the second difference is computed as
        <m>14.26 - 13.55 = 0.71</m>, and the third is <m>13.50 - 12.53 = 0.97</m>. A histogram of the
        differences is shown in Figure<nbsp/><xref ref="diffInTextbookPricesF18"/>. Using differences
        between paired observations is a common and useful way to analyze paired data.
      </p>
      
      <figure xml:id="diffInTextbookPricesF18">
        <caption>Histogram of the difference in price for each book sampled.</caption>
        <p>
          <!-- TODO: Add figure - A histogram is shown for "UCLA bookstore Price minus Amazon Price, 
          in US dollars", where values range from -$20 to positive $80. The distribution has a prominent 
          peak at or slightly above $0, with the wide majority of data lying between -$20 and positive $20. 
          There are also 4 bins above $20 that have non-zero heights: bin $20 to $30 has a height of 2, 
          bin $30 to $40 has a height of 2, bin $50 to $60 has a height of 1, and bin $70 to $80 has a 
          height of 1. -->
        </p>
      </figure>
    </subsection>
    
    <subsection xml:id="inference-for-paired-data">
      <title>Inference for paired data</title>
      
      <p>
        To analyze a paired data set, we simply analyze the differences. We can use the same
        <m>t</m>-distribution techniques we applied in Section<nbsp/><xref ref="oneSampleMeansWithTDistribution"/>.
      </p>
      
      <figure xml:id="textbooksSummaryStats">
        <caption>Summary statistics for the 68 price differences.</caption>
        <tabular>
          <row bottom="medium">
            <cell><m>n_{\text{diff}}</m></cell>
            <cell></cell>
            <cell><m>\bar{x}_{\text{diff}}</m></cell>
            <cell></cell>
            <cell><m>s_{\text{diff}}</m></cell>
          </row>
          <row>
            <cell>68</cell>
            <cell></cell>
            <cell>3.58</cell>
            <cell></cell>
            <cell>13.42</cell>
          </row>
        </tabular>
      </figure>
      
      <example xml:id="htSetupTextbookPriceDiff">
        <statement>
          <p>
            Set up a hypothesis test to determine whether, on average, there is a difference between
            Amazon's price for a book and the UCLA bookstore's price. Also, check the conditions for
            whether we can move forward with the test using the <m>t</m>-distribution.
          </p>
        </statement>
        <solution>
          <p>
            We are considering two scenarios: there is no difference or there is some difference in
            average prices.
          </p>
          <ul>
            <li><m>H_0</m>: <m>\mu_{\text{diff}} = 0</m>. There is no difference in the average
            textbook price.</li>
            <li><m>H_A</m>: <m>\mu_{\text{diff}} \neq 0</m>. There is a difference in average prices.</li>
          </ul>
          <p>
            Next, we check the independence and normality conditions. The observations are based on a
            simple random sample, so independence is reasonable. While there are some outliers,
            <m>n = 68</m> and none of the outliers are particularly extreme, so the normality of
            <m>\bar{x}</m> is satisfied. With these conditions satisfied, we can move forward with the
            <m>t</m>-distribution.
          </p>
        </solution>
      </example>
      
      <example xml:id="SEAndTScoreTextbookPriceDiff">
        <statement>
          <p>
            Complete the hypothesis test started in Example<nbsp/><xref ref="htSetupTextbookPriceDiff"/>.
          </p>
        </statement>
        <solution>
          <p>
            To compute the test statistic, we compute the standard error associated with
            <m>\bar{x}_{\text{diff}}</m> using the standard deviation of the differences
            (<m>s_{\text{diff}} = 13.42</m>) and the number of differences
            (<m>n_{\text{diff}} = 68</m>):
          </p>
          <me>
            SE_{\bar{x}_{\text{diff}}} = \frac{s_{\text{diff}}}{\sqrt{n_{\text{diff}}}} 
            = \frac{13.42}{\sqrt{68}} = 1.63
          </me>
          <p>
            The test statistic is the T-score of <m>\bar{x}_{\text{diff}}</m> under the null condition
            that the actual mean difference is 0:
          </p>
          <me>
            T = \frac{\bar{x}_{\text{diff}} - 0}{SE_{\bar{x}_{\text{diff}}}} 
            = \frac{3.58 - 0}{1.63} = 2.20
          </me>
          <p>
            To visualize the p-value, the sampling distribution of <m>\bar{x}_{\text{diff}}</m> is
            drawn as though <m>H_0</m> is true, and the p-value is represented by the two shaded tails:
          </p>
          <p>
            <!-- TODO: Add figure - A bell-shaped distribution is shown, with a center of mu-sub-0, 
            which has a value of 0. The area under the distribution above x-bar-sub-diff equals 3.58 
            is shaded, as is the corresponding tail below -3.58. -->
          </p>
          <p>
            The degrees of freedom is <m>df = 68 - 1 = 67</m>. Using statistical software, we find the
            one-tail area of 0.0156. Doubling this area gives the p-value: 0.0312.
          </p>
          <p>
            Because the p-value is less than 0.05, we reject the null hypothesis. Amazon prices are,
            on average, lower than the UCLA Bookstore prices for UCLA courses.
          </p>
        </solution>
      </example>
      
      <exercise>
        <statement>
          <p>
            Create a 95% confidence interval for the average price difference between books at the UCLA
            bookstore and books on Amazon.<fn>Conditions have already been verified and the standard
            error computed in Example<nbsp/><xref ref="htSetupTextbookPriceDiff"/>. To find the interval,
            identify <m>t^{\star}_{67}</m> using statistical software or the <m>t</m>-table
            (<m>t^{\star}_{67} = 2.00</m>), and plug it, the point estimate, and the standard error into
            the confidence interval formula: <m>\text{point estimate} \pm t^{\star} \times SE \to 3.58 \pm 2.00 \times 1.63 \to (0.32, 6.84)</m>. We are 95% confident that Amazon is, on average,
            between $0.32 and $6.84 less expensive than the UCLA Bookstore for UCLA course books.</fn>
          </p>
        </statement>
      </exercise>
      
      <exercise>
        <statement>
          <p>
            We have strong evidence that Amazon is, on average, less expensive. How should this
            conclusion affect UCLA student buying habits? Should UCLA students always buy their books on
            Amazon?<fn>The average price difference is only mildly useful for this question. Examine the
            distribution shown in Figure<nbsp/><xref ref="diffInTextbookPricesF18"/>. There are certainly
            a handful of cases where Amazon prices are far below the UCLA Bookstore's, which suggests it
            is worth checking Amazon (and probably other online sites) before purchasing. However, in many
            cases the Amazon price is above what the UCLA Bookstore charges, and most of the time the
            price isn't that different. Ultimately, if getting a book immediately from the bookstore is
            notably more convenient, e.g. to get started on reading or homework, it's likely a good idea
            to go with the UCLA Bookstore unless the price difference on a specific book happens to be
            quite large. For reference, this is a very different result from what we (the authors) had
            seen in a similar data set from 2010. At that time, Amazon prices were almost uniformly lower
            than those of the UCLA Bookstore's and by a large margin, making the case to use Amazon over
            the UCLA Bookstore quite compelling at that time. Now we frequently check multiple websites
            to find the best price.</fn>
          </p>
        </statement>
      </exercise>
    </subsection>
    
    <exercises>
      <title>Section Exercises</title>
      
      <exercise xml:id="air_quality_shortened">
        <title>Air quality</title>
        <statement>
          <p>
            Air quality measurements were collected in a random sample of 25 country capitals in 2013,
            and then again in the same cities in 2014. We would like to use these data to compare average
            air quality between the two years. Should we use a paired or non-paired test? Explain your
            reasoning.
          </p>
        </statement>
      </exercise>
      
      <exercise xml:id="tf_paired">
        <title>True / False: paired</title>
        <statement>
          <p>
            Determine if the following statements are true or false. If false, explain.
          </p>
          <ol>
            <li>In a paired analysis we first take the difference of each pair of observations, and then
            we do inference on these differences.</li>
            <li>Two data sets of different sizes cannot be analyzed as paired data.</li>
            <li>Consider two sets of data that are paired with each other. Each observation in one data
            set has a natural correspondence with exactly one observation from the other data set.</li>
            <li>Consider two sets of data that are paired with each other. Each observation in one data
            set is subtracted from the average of the other data set's observations.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="paired_or_not_1">
        <title>Paired or not? Part I</title>
        <statement>
          <p>
            In each of the following scenarios, determine if the data are paired.
          </p>
          <ol>
            <li>Compare pre- (beginning of semester) and post-test (end of semester) scores of students.</li>
            <li>Assess gender-related salary gap by comparing salaries of randomly sampled men and women.</li>
            <li>Compare artery thicknesses at the beginning of a study and after 2 years of taking Vitamin
            E for the same group of patients.</li>
            <li>Assess effectiveness of a diet regimen by comparing the before and after weights of subjects.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="paired_or_not_2">
        <title>Paired or not? Part II</title>
        <statement>
          <p>
            In each of the following scenarios, determine if the data are paired.
          </p>
          <ol>
            <li>We would like to know if Intel's stock and Southwest Airlines' stock have similar rates
            of return. To find out, we take a random sample of 50 days, and record Intel's and Southwest's
            stock on those same days.</li>
            <li>We randomly sample 50 items from Target stores and note the price for each. Then we visit
            Walmart and collect the price for each of those same 50 items.</li>
            <li>A school board would like to determine whether there is a difference in average SAT scores
            for students at one high school versus another high school in the district. To check, they take
            a simple random sample of 100 students from each high school.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="global_warming_v2_1">
        <title>Global warming, Part I</title>
        <statement>
          <p>
            Let's consider a limited set of climate data, examining temperature differences in 1948 vs
            2018. We sampled 197 locations from the National Oceanic and Atmospheric Administration's
            (NOAA) historical data, where the data was available for both years of interest. We want to
            know: were there more days with temperatures exceeding 90Â°F in 2018 or in 1948? The difference
            in number of days exceeding 90Â°F (number of days in 2018 - number of days in 1948) was
            calculated for each of the 197 locations. The average of these differences was 2.9 days with a
            standard deviation of 17.2 days. We are interested in determining whether these data provide
            strong evidence that there were more days in 2018 that exceeded 90Â°F from NOAA's weather
            stations.
          </p>
          <p>
            <!-- TODO: Add histogram figure - A histogram is shown for "Differences in Number of Days", 
            which has bins between -70 and 60, where the bin width is 10. There is a prominent peak around 
            zero, where much of the data lies between -40 and positive 40. The non-zero bins beyond this 
            range are -70 to -60 has a bin height of 1, the 40 to 50 bin has a bin height of 2, and the 50 
            to 60 bin has a bin height of 1. -->
          </p>
          <ol>
            <li>Is there a relationship between the observations collected in 1948 and 2018? Or are the
            observations in the two groups independent? Explain.</li>
            <li>Write hypotheses for this research in symbols and in words.</li>
            <li>Check the conditions required to complete this test. A histogram of the differences is
            given to the right.</li>
            <li>Calculate the test statistic and find the p-value.</li>
            <li>Use <m>\alpha = 0.05</m> to evaluate the test, and interpret your conclusion in context.</li>
            <li>What type of error might we have made? Explain in context what the error means.</li>
            <li>Based on the results of this hypothesis test, would you expect a confidence interval for
            the average difference between the number of days exceeding 90Â°F from 1948 and 2018 to include
            0? Explain your reasoning.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="hs_beyond_1">
        <title>High School and Beyond, Part I</title>
        <statement>
          <p>
            The National Center of Education Statistics conducted a survey of high school seniors,
            collecting test data on reading, writing, and several other subjects. Here we examine a simple
            random sample of 200 students from this survey. Side-by-side box plots of reading and writing
            scores as well as a histogram of the differences in scores are shown below.
          </p>
          <p>
            <!-- TODO: Add figures - Side-by-side box plot with dot plots also overlaid for each box plot. 
            There are two categories shown, "read" and "write", for values ranging from about 27 to 77. The 
            box portion of each distribution is nearly identical, ranging from about 45 to 60. The median of 
            "read" is about 49 while the median of "write" is about 53. The whiskers for "read" extend down 
            to about 27 and up to 77, while the whiskers for "write" extend down to about 32 and up to about 
            67. No points are shown beyond the whiskers for either box plot. -->
          </p>
          <p>
            <!-- TODO: Add histogram - A histogram is shown for "Difference in scores (read minus write)", 
            which is centered at approximately zero and is roughly bell-shaped with values ranging from -25 
            to positive 25. -->
          </p>
          <ol>
            <li>Is there a clear difference in the average reading and writing scores?</li>
            <li>Are the reading and writing scores of each student independent of each other?</li>
            <li>Create hypotheses appropriate for the following research question: is there an evident
            difference in the average scores of students in the reading and writing exam?</li>
            <li>Check the conditions required to complete this test.</li>
            <li>The average observed difference in scores is <m>\bar{x}_{\text{read-write}} = -0.545</m>,
            and the standard deviation of the differences is 8.887 points. Do these data provide convincing
            evidence of a difference between the average scores on the two exams?</li>
            <li>What type of error might we have made? Explain what the error means in the context of the
            application.</li>
            <li>Based on the results of this hypothesis test, would you expect a confidence interval for
            the average difference between the reading and writing scores to include 0? Explain your
            reasoning.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="global_warming_v2_2">
        <title>Global warming, Part II</title>
        <statement>
          <p>
            We considered the change in the number of days exceeding 90Â°F from 1948 and 2018 at 197
            randomly sampled locations from the NOAA database in Exercise<nbsp/><xref ref="global_warming_v2_1"/>.
            The mean and standard deviation of the reported differences are 2.9 days and 17.2 days.
          </p>
          <ol>
            <li>Calculate a 90% confidence interval for the average difference between number of days
            exceeding 90Â°F between 1948 and 2018. We've already checked the conditions for you.</li>
            <li>Interpret the interval in context.</li>
            <li>Does the confidence interval provide convincing evidence that there were more days exceeding
            90Â°F in 2018 than in 1948 at NOAA stations? Explain.</li>
          </ol>
        </statement>
      </exercise>
      
      <exercise xml:id="hs_beyond_2">
        <title>High school and beyond, Part II</title>
        <statement>
          <p>
            We considered the differences between the reading and writing scores of a random sample of 200
            students who took the High School and Beyond Survey in Exercise<nbsp/><xref ref="hs_beyond_1"/>.
            The mean and standard deviation of the differences are <m>\bar{x}_{\text{read-write}} = -0.545</m>
            and 8.887 points.
          </p>
          <ol>
            <li>Calculate a 95% confidence interval for the average difference between the reading and
            writing scores of all students.</li>
            <li>Interpret this interval in context.</li>
            <li>Does the confidence interval provide convincing evidence that there is a real difference
            in the average scores? Explain.</li>
          </ol>
        </statement>
      </exercise>
    </exercises>
  </section>
  
  <!-- Section 6.3: Difference of two means -->
  <section xml:id="sec-difference-two-means">
    <title>Difference of Two Means</title>
    
    <introduction>
      <p>
        We now consider a different scenario: comparing means from two independent groups. For
        example, we might compare average exam scores between students who attended review sessions
        and those who didn't, or compare average recovery times between patients receiving two
        different treatments.
      </p>
    </introduction>
    
    <subsection xml:id="subsec-comparing-two-indep-means">
      <title>Comparing Two Independent Means</title>
      
      <p>
        When comparing two independent groups, we examine the difference in sample means:
        <m>\bar{x}_1 - \bar{x}_2</m>. This quantity estimates the difference in population means:
        <m>\mu_1 - \mu_2</m>.
      </p>
      
      <definition xml:id="def-two-sample-conditions">
        <statement>
          <p>
            For inference on the difference of two means, the following conditions should be met:
          </p>
          <ol>
            <li><alert>Independence:</alert> Within each group, observations must be independent.
                The two groups must also be independent of each other.</li>
            <li><alert>Normality:</alert> The data in each group should come from a nearly normal
                distribution, or each sample size should be sufficiently large.</li>
          </ol>
        </statement>
      </definition>
    </subsection>
    
    <subsection xml:id="subsec-two-sample-t-procedures">
      <title>Two-Sample <m>t</m>-Procedures</title>
      
      <p>
        The standard error for the difference of two independent sample means is:
      </p>
      
      <md>
        SE = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
      </md>
      
      <p>
        The degrees of freedom calculation for the two-sample t-test is complex. Most software
        uses the Welch-Satterthwaite approximation. A conservative approach is to use
        <m>df = \min(n_1 - 1, n_2 - 1)</m>.
      </p>
      
      <p>
        <alert>Confidence interval for <m>\mu_1 - \mu_2</m>:</alert>
      </p>
      
      <md>
        (\bar{x}_1 - \bar{x}_2) \pm t^*_{df} \times \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
      </md>
      
      <p>
        <alert>Test statistic for <m>H_0: \mu_1 = \mu_2</m>:</alert>
      </p>
      
      <md>
        t = \frac{(\bar{x}_1 - \bar{x}_2) - 0}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
      </md>
      
      <procedure xml:id="proc-two-sample-t">
        <title>Two-Sample <m>t</m>-Procedures</title>
        <step>
          <title>Prepare</title>
          <p>
            Identify summary statistics for both groups and determine the parameter of interest.
          </p>
        </step>
        <step>
          <title>Check</title>
          <p>
            Verify independence within and between groups, and check the normality condition for
            each group.
          </p>
        </step>
        <step>
          <title>Calculate</title>
          <p>
            Compute the standard error and degrees of freedom. Calculate the confidence interval
            or test statistic as appropriate.
          </p>
        </step>
        <step>
          <title>Conclude</title>
          <p>
            Interpret the results in context.
          </p>
        </step>
      </procedure>
    </subsection>
    
    <subsection xml:id="subsec-pooled-standard-deviation">
      <title>Pooled Standard Deviation (Optional)</title>
      
      <p>
        When we have strong reason to believe that the two populations have equal variances, we
        can use a <term>pooled standard deviation</term> to get a more precise estimate. The pooled
        standard deviation is:
      </p>
      
      <md>
        s_{pooled} = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}
      </md>
      
      <p>
        The standard error becomes <m>SE = s_{pooled}\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}</m> and
        the degrees of freedom is <m>df = n_1 + n_2 - 2</m>. However, this approach should only be
        used when the equal variance assumption is reasonable.
      </p>
    </subsection>
  </section>
  
  <!-- Section 6.4: Power calculations for difference of means -->
  <section xml:id="sec-power-calculations">
    <title>Power Calculations for a Difference of Means</title>
    
    <introduction>
      <p>
        When planning a study, researchers often want to know: How large should my sample be to
        detect a meaningful effect? This question relates to the concept of <term>statistical power</term>.
      </p>
    </introduction>
    
    <subsection xml:id="subsec-statistical-power">
      <title>Understanding Statistical Power</title>
      
      <definition xml:id="def-power">
        <statement>
          <p>
            The <term>power</term> of a hypothesis test is the probability that the test correctly
            rejects a false null hypothesis. In other words, it's the probability of detecting an
            effect when one truly exists.
          </p>
          <md>
            \text{Power} = 1 - P(\text{Type 2 Error}) = 1 - \beta
          </md>
        </statement>
      </definition>
      
      <p>
        Power depends on several factors:
      </p>
      
      <ul>
        <li>The <alert>significance level</alert> <m>\alpha</m> (lower <m>\alpha</m> means lower power)</li>
        <li>The <alert>effect size</alert> (larger effects are easier to detect)</li>
        <li>The <alert>sample size</alert> (larger samples provide more power)</li>
        <li>The <alert>variability</alert> in the data (less variability means more power)</li>
      </ul>
      
      <p>
        Researchers typically aim for a power of 0.80 or higher, meaning an 80% chance of detecting
        a true effect.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-sample-size-determination">
      <title>Sample Size Determination</title>
      
      <p>
        Power calculations can be used to determine the necessary sample size for a study. The
        process involves specifying:
      </p>
      
      <ol>
        <li>The desired significance level <m>\alpha</m></li>
        <li>The desired power (typically 0.80)</li>
        <li>The minimum effect size you want to detect</li>
        <li>An estimate of the population standard deviation</li>
      </ol>
      
      <p>
        With these inputs, statistical software or formulas can calculate the required sample size
        for each group. Adequate planning using power calculations helps ensure studies are neither
        underpowered (unable to detect real effects) nor wastefully large.
      </p>
    </subsection>
  </section>
  
  <!-- Section 6.5: Comparing many means with ANOVA -->
  <section xml:id="sec-anova">
    <title>Comparing Many Means with ANOVA</title>
    
    <introduction>
      <p>
        Sometimes we want to compare means across more than two groups. For example, we might want
        to compare average test scores across four different teaching methods, or compare recovery
        times across three different treatments. When comparing multiple groups, we use
        <term>Analysis of Variance (ANOVA)</term>.
      </p>
    </introduction>
    
    <subsection xml:id="subsec-anova-hypotheses">
      <title>ANOVA Hypotheses and Conditions</title>
      
      <p>
        Consider comparing <m>k</m> groups with means <m>\mu_1, \mu_2, \ldots, \mu_k</m>. The
        hypotheses for ANOVA are:
      </p>
      
      <ul>
        <li><m>H_0</m>: The mean outcome is the same across all groups.
            <m>\mu_1 = \mu_2 = \cdots = \mu_k</m></li>
        <li><m>H_A</m>: At least one mean is different from the others.</li>
      </ul>
      
      <definition xml:id="def-anova-conditions">
        <statement>
          <p>
            <term>Conditions for ANOVA:</term>
          </p>
          <ol>
            <li><alert>Independence:</alert> Observations must be independent within and across groups.</li>
            <li><alert>Normality:</alert> The data within each group should be approximately normal.</li>
            <li><alert>Equal variance:</alert> The variability should be roughly constant across groups.</li>
          </ol>
        </statement>
      </definition>
    </subsection>
    
    <subsection xml:id="subsec-f-statistic">
      <title>The <m>F</m>-Statistic</title>
      
      <p>
        ANOVA uses the <term>F-statistic</term> to compare group means. The F-statistic is a ratio
        of two measures of variability:
      </p>
      
      <md>
        F = \frac{\text{Variability between groups}}{\text{Variability within groups}} = \frac{MSG}{MSE}
      </md>
      
      <p>
        where MSG is the <term>mean square between groups</term> and MSE is the
        <term>mean square error</term> (within groups).
      </p>
      
      <ul>
        <li>If the null hypothesis is true (all means equal), we expect <m>F \approx 1</m>.</li>
        <li>If at least one mean is different, we expect <m>F > 1</m>.</li>
        <li>Large values of <m>F</m> provide evidence against <m>H_0</m>.</li>
      </ul>
      
      <definition xml:id="def-f-distribution">
        <statement>
          <p>
            The <term>F-distribution</term> is a right-skewed distribution (starting at 0) used for
            ANOVA. It has two degrees of freedom parameters:
          </p>
          <ul>
            <li><m>df_1 = k - 1</m> (degrees of freedom for groups, where <m>k</m> is the number of groups)</li>
            <li><m>df_2 = n - k</m> (degrees of freedom for error, where <m>n</m> is the total sample size)</li>
          </ul>
        </statement>
      </definition>
    </subsection>
    
    <subsection xml:id="subsec-anova-table">
      <title>The ANOVA Table</title>
      
      <p>
        ANOVA results are typically summarized in an <term>ANOVA table</term>:
      </p>
      
      <p>
        <alert>ANOVA Table Structure:</alert>
      </p>
      
      <tabular>
        <row header="yes">
          <cell>Source</cell>
          <cell>Sum of Squares</cell>
          <cell>df</cell>
          <cell>Mean Square</cell>
          <cell>F</cell>
          <cell>p-value</cell>
        </row>
        <row>
          <cell>Groups</cell>
          <cell>SSG</cell>
          <cell><m>k-1</m></cell>
          <cell><m>MSG = \frac{SSG}{k-1}</m></cell>
          <cell><m>\frac{MSG}{MSE}</m></cell>
          <cell></cell>
        </row>
        <row>
          <cell>Error</cell>
          <cell>SSE</cell>
          <cell><m>n-k</m></cell>
          <cell><m>MSE = \frac{SSE}{n-k}</m></cell>
          <cell></cell>
          <cell></cell>
        </row>
        <row>
          <cell>Total</cell>
          <cell>SST</cell>
          <cell><m>n-1</m></cell>
          <cell></cell>
          <cell></cell>
          <cell></cell>
        </row>
      </tabular>
    </subsection>
    
    <subsection xml:id="subsec-multiple-comparisons">
      <title>Multiple Comparisons and What ANOVA Doesn't Tell Us</title>
      
      <p>
        When ANOVA gives a significant result, it tells us that at least one mean is different, but
        it doesn't tell us which means differ. To determine which specific groups differ, we need to
        conduct <term>multiple comparisons</term> or <term>post-hoc tests</term>.
      </p>
      
      <important>
        <p>
          <alert>Multiple Testing Problem:</alert> When conducting many pairwise comparisons, the
          chance of making at least one Type 1 error increases. Methods like the
          <term>Bonferroni correction</term> or <term>Tukey's HSD</term> help control this error rate.
        </p>
      </important>
      
      <p>
        A simple approach is the <term>Bonferroni correction</term>: If conducting <m>m</m> tests,
        use <m>\alpha/m</m> as the significance level for each individual test to maintain an
        overall significance level of approximately <m>\alpha</m>.
      </p>
    </subsection>
  </section>
  
  <!-- Section 6.6: Chapter review -->
  <section xml:id="sec-ch06-review">
    <title>Chapter 6 Review Exercises</title>
    
    <p>
      This chapter introduced inference for numerical data using the t-distribution. Key concepts include:
    </p>
    
    <ul>
      <li>The t-distribution and its properties</li>
      <li>One-sample t-confidence intervals and hypothesis tests</li>
      <li>Paired data analysis using differences</li>
      <li>Two-sample t-procedures for comparing independent groups</li>
      <li>Statistical power and sample size determination</li>
      <li>ANOVA for comparing three or more means</li>
    </ul>
    
    <p>
      Additional exercises for practicing these concepts are available in the accompanying
      exercise materials.
    </p>
  </section>
</chapter>
